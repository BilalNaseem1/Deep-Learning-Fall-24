# Deep Learning by Dr. Tahir Syed - Fall 2024

## Lectures
1. Introduction
    - [Lecture 1a - Origins of Deep Learning](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-1a.pdf)
    - [Lecture 1b - The Perceptron](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-1b.pdf)
2. Neural Nets As Universal Approximators
    - [Lecture 2 - Neural Networks as Universal Approximators](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-2.pdf)
3. Empirical Risk Minimization
    - [Lecture 3 - Empirical Risk Minimization](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-3.pdf)
4. Gradient Descent & Loss Functions
    - [Lecture 4a - Gradient Descent](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-4a.pdf)
    - [Lecture 4b - Loss Functions](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-4b.pdf)
5. Backpropagation
    - [Lecture 5a - Calculus Refresher + Forward Pass](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-5a.pdf)
    - [Lecture 5b - Backward Pass](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-5b.pdf)
    - [Lecture 5c - Softmax in depth + ReLU and Subgradients](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/lecture_notes/lecture-5c.pdf)
    - [Lecture 5d - Vector Formulation]
    - [Computational Graph Practice Problems for Mid-Term](https://cs230.stanford.edu/winter2020/section3_exercises.pdf)


### Remaining Topics for Mid-Term
These are the remaining topics for Mid-Term. Recommended resources to Cover these are [CV course by UMich - L7, 8, 10, 11](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r) and [Notes by Matt D. of CS231n](https://mattdeitke.com/notes/cs231n#[53,%22XYZ%22,129.6,307.097,null]) and [DL UC Berkeley Lecture 6 and 7 all parts](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)

1. CNN
2. Hyperbolic Activation Functions, Leaky ReLU ..
3. Data Pre-processing
4. Weight Initialization
5. Batch Normalization
6. Regularization (Dropout and Data Augmentation)
7. Specific CNN Architectures - DL UC Berkeley Lecture 6 part 3


## Labs
1. OOP Fundamentals
    - [Lab 1 - OOP Tutorial](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/labs/lab-1-OOP-fundamentals.ipynb)
    - [OOP Homework](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/labs/lab-1-OOP-fundamentals-hw.md)
2. Vectors and Tensors
    - [Lab 2a - Vectors and Tensors using Numpy - Tutorial](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/labs/lab-2-Numpy-Vectors-and-Tensors.ipynb)
    - [Lab 2b - Tensors using Torch (colab) - Tutorial](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/labs/lab-2b-tensors-in-torch.ipynb)
    - [Lab 2c - Working with Tensors using GPU (colab) - Tutorial](https://github.com/BilalNaseem1/Deep-Learning-Fall-24/blob/main/labs/lab-2c-torch-tensors-in-gpu.ipynb)
3. Introduction to PyTorch
    - Lab 3a - AutoGrad
4. Running the Model on AWS
5. Datasets
6. Dataloaders
7. Data Preprocessing
8. Wandb
9. Losses
10. Block Processing
11. Pipelines